from typing import List, Optional, AsyncGenerator, Set
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from sqlalchemy.orm import selectinload
from sqlalchemy.sql import text
import json
import re
import asyncio
from datetime import datetime

from app.models.chat import Chat as ChatModel, Message as MessageModel, Tag, DocumentTag
from app.schemas.chat import ChatCreate, MessageCreate, ChatListItem, Chat, Message
from app.services.azure_openai import azure_openai_service
from app.services.embedding_service import embedding_service
from app.services.deep_research_service import DeepResearchService


class ChatService:
    @staticmethod
    def strip_system_context_tags(content: str) -> str:
        """Remove system context tags from content for frontend display"""
        import re
        # Remove <system_context>...</system_context> tags and their content
        pattern = r'<system_context>.*?</system_context>\s*\n*'
        return re.sub(pattern, '', content, flags=re.DOTALL).strip()
    
    @staticmethod
    def extract_tags_from_message(message: str) -> Set[str]:
        """Extract tag references from a message (e.g., #tagname or [tag:tagname])"""
        tags = set()
        
        # Pattern 1: #tagname (alphanumeric and underscore, dash)
        hashtag_pattern = r'#([\w-]+)'
        tags.update(re.findall(hashtag_pattern, message))
        
        # Pattern 2: [tag:tagname]
        bracket_pattern = r'\[tag:([\w-]+)\]'
        tags.update(re.findall(bracket_pattern, message))
        
        return tags
    
    @staticmethod
    def extract_document_references_from_message(message: str) -> Set[str]:
        """Extract document references from a message (e.g., /doc "Document Name" or /doc DocumentName)"""
        documents = set()
        
        # Pattern 1: /doc "Document Name" or /document "Document Name"
        quoted_pattern = r'/(?:doc|document)\s+"([^"]+)"'
        documents.update(re.findall(quoted_pattern, message))
        
        # Pattern 2: /doc DocumentName or /document DocumentName (single word)
        unquoted_pattern = r'/(?:doc|document)\s+([^\s]+)'
        unquoted_matches = re.findall(unquoted_pattern, message)
        # Filter out matches that are already quoted (to avoid duplicates)
        for match in unquoted_matches:
            if not match.startswith('"'):
                documents.add(match)
        
        return documents
    
    @staticmethod
    async def get_tag_ids_by_names(db: AsyncSession, tag_names: Set[str]) -> List[UUID]:
        """Get tag IDs from tag names"""
        if not tag_names:
            return []

        # Convert to lowercase for case-insensitive matching
        lower_names = [name.lower() for name in tag_names]

        result = await db.execute(
            select(Tag.id).where(
                Tag.name.in_(tag_names) |
                Tag.name.in_(lower_names)
            )
        )
        return list(result.scalars().all())
    
    @staticmethod
    async def get_document_ids_by_titles(db: AsyncSession, document_titles: Set[str]) -> List[UUID]:
        """Get document IDs from document titles"""
        if not document_titles:
            return []

        from app.models.chat import Document
        from sqlalchemy import func, or_

        # Build conditions for case-insensitive matching on both title and filename
        conditions = []
        for title in document_titles:
            conditions.extend([
                func.lower(Document.title) == title.lower(),
                func.lower(Document.filename) == title.lower()
            ])

        result = await db.execute(
            select(Document.id).where(
                or_(*conditions)
            )
        )
        return list(result.scalars().all())
    @staticmethod
    async def create_chat(db: AsyncSession, chat_data: ChatCreate) -> Chat:
        """Create a new chat"""
        db_chat = ChatModel(title=chat_data.title)
        db.add(db_chat)
        await db.commit()
        await db.refresh(db_chat)
        return db_chat
    
    @staticmethod
    async def get_chat(db: AsyncSession, chat_id: UUID) -> Optional[Chat]:
        """Get a chat by ID with messages"""
        result = await db.execute(
            select(ChatModel)
            .options(selectinload(ChatModel.messages))
            .where(ChatModel.id == chat_id)
        )
        return result.scalar_one_or_none()
    
    @staticmethod
    async def get_chats(db: AsyncSession, limit: int = 50) -> List[ChatListItem]:
        """Get all chats with basic info"""
        # Get chats with their latest message
        query = """
        SELECT
            c.id,
            c.title,
            c.created_at,
            c.updated_at,
            (
                SELECT m.content
                FROM messages m
                WHERE m.chat_id = c.id
                ORDER BY m.created_at DESC
                LIMIT 1
            ) as last_message,
            (
                SELECT COUNT(*)
                FROM messages m
                WHERE m.chat_id = c.id
            ) as message_count
        FROM chats c
        ORDER BY c.updated_at DESC
        LIMIT :limit
        """

        result = await db.execute(text(query), {"limit": limit})
        rows = result.fetchall()
        
        return [
            ChatListItem(
                id=row.id,
                title=row.title,
                created_at=row.created_at,
                updated_at=row.updated_at,
                last_message=row.last_message,
                message_count=row.message_count
            )
            for row in rows
        ]
    
    @staticmethod
    async def update_chat(db: AsyncSession, chat_id: UUID, chat_data) -> Optional[Chat]:
        """Update a chat's title and/or messages"""
        # Get the chat
        result = await db.execute(
            select(ChatModel)
            .options(selectinload(ChatModel.messages))
            .where(ChatModel.id == chat_id)
        )
        chat = result.scalar_one_or_none()
        
        if not chat:
            return None
        
        # Update title if provided
        if chat_data.title is not None:
            chat.title = chat_data.title
        
        # Update messages if provided
        if chat_data.messages is not None:
            # Delete existing messages
            await db.execute(
                text("DELETE FROM messages WHERE chat_id = :chat_id"),
                {"chat_id": str(chat_id)}
            )
            
            # Add new messages
            for idx, msg_data in enumerate(chat_data.messages):
                message = MessageModel(
                    chat_id=chat_id,
                    content=msg_data.get('content', ''),
                    role=msg_data.get('role', 'user'),
                    token_count=len(msg_data.get('content', '').split())  # Simple token count
                )
                db.add(message)
        
        await db.commit()
        await db.refresh(chat)
        
        # Return the updated chat with messages
        return Chat(
            id=chat.id,
            title=chat.title,
            created_at=chat.created_at,
            updated_at=chat.updated_at,
            messages=[
                Message(
                    id=msg.id,
                    chat_id=msg.chat_id,
                    content=ChatService.strip_system_context_tags(msg.content),
                    role=msg.role,
                    created_at=msg.created_at,
                    token_count=msg.token_count
                )
                for msg in chat.messages
            ]
        )
    
    @staticmethod
    async def delete_chat(db: AsyncSession, chat_id: UUID) -> bool:
        """Delete a chat and all its messages"""
        result = await db.execute(
            select(ChatModel).where(ChatModel.id == chat_id)
        )
        chat = result.scalar_one_or_none()
        
        if chat:
            await db.delete(chat)
            await db.commit()
            return True
        return False
    
    @staticmethod
    async def add_message(
        db: AsyncSession, 
        chat_id: UUID, 
        message_data: MessageCreate
    ) -> Message:
        """Add a message to a chat"""
        # Count tokens
        token_count = azure_openai_service.count_tokens(message_data.content)
        
        db_message = MessageModel(
            chat_id=chat_id,
            content=message_data.content,
            role=message_data.role,
            token_count=token_count
        )
        
        db.add(db_message)
        await db.commit()
        await db.refresh(db_message)
        
        # Update chat's updated_at timestamp
        await db.execute(
            select(ChatModel).where(ChatModel.id == chat_id)
        )
        chat_result = await db.execute(select(ChatModel).where(ChatModel.id == chat_id))
        chat = chat_result.scalar_one_or_none()
        if chat:
            await db.commit()  # This will trigger the onupdate for updated_at
        
        return db_message
    
    @staticmethod
    async def generate_response(
        db: AsyncSession,
        chat_id: UUID,
        user_message: str,
        use_rag: bool = True,
        rag_limit: int = 5,
        rag_threshold: float = 0.7,
        use_deep_research: bool = False,
        max_concurrent_research_units: int = 1,
        max_researcher_iterations: int = 1,
        max_react_tool_calls: int = 1,
        max_structured_output_retries: int = 1
    ) -> AsyncGenerator[tuple[str, Optional[UUID]], None]:
        """Generate AI response for a chat message with optional RAG support"""
        # Get chat with messages for context
        chat = await ChatService.get_chat(db, chat_id)
        if not chat:
            raise ValueError("Chat not found")
        
        # Add user message (store original without system context tags)
        user_msg = await ChatService.add_message(
            db, 
            chat_id, 
            MessageCreate(content=user_message, role="user")
        )
        
        # Extract tags from user message
        tag_names = ChatService.extract_tags_from_message(user_message)
        tag_ids = await ChatService.get_tag_ids_by_names(db, tag_names) if tag_names else []
        
        # Extract document references from user message
        document_titles = ChatService.extract_document_references_from_message(user_message)
        document_ids = await ChatService.get_document_ids_by_titles(db, document_titles) if document_titles else []
        
        # Perform similarity search to retrieve relevant context if RAG is enabled
        relevant_chunks = []
        if use_rag:
            relevant_chunks = await embedding_service.similarity_search(
                db=db,
                query=user_message,
                limit=rag_limit,
                similarity_threshold=rag_threshold,
                tag_ids=tag_ids if tag_ids else None,
                document_ids=document_ids if document_ids else None
            )
            print(f"\nTHESE ARE THE RELEVANT CHUNKS: {relevant_chunks}!")
            if tag_names:
                print(f"FILTERED BY TAGS: {tag_names}")
                print(f"TAG IDS FOUND: {tag_ids}")
            if document_titles:
                print(f"FILTERED BY DOCUMENTS: {document_titles}")
                print(f"DOCUMENT IDS FOUND: {document_ids}")
        
        # Build context from retrieved chunks and prepare citation mapping
        context_parts = []
        chunk_to_citation_mapping = {}  # Maps citation numbers to chunk details
        
        if relevant_chunks:
            # Check if we're using fallback results (all chunks have distance > threshold)
            threshold_distance = 1.0 - rag_threshold
            using_fallback = all(getattr(chunk, 'search_distance', 0) >= threshold_distance for chunk in relevant_chunks)
            
            if using_fallback:
                context_parts.append("## Related Knowledge from Database (Less Relevant):\n")
                context_parts.append("*Note: No highly relevant matches found. The following are the closest available documents:*\n\n")
            else:
                context_parts.append("## Relevant Knowledge from Database:\n")
            
            for i, chunk in enumerate(relevant_chunks, 1):
                # Parse metadata to get source information
                metadata = json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}
                source_info = f"[Source: {getattr(chunk, 'document_title', 'Unknown')}]"
                distance = getattr(chunk, 'search_distance', 'unknown')
                
                if isinstance(distance, float):
                    confidence = f" (Similarity: {(1.0 - distance) * 100:.1f}%)"
                else:
                    confidence = ""
                
                # Store citation mapping for later use
                chunk_to_citation_mapping[i] = {
                    'chunk_id': str(chunk.id),
                    'document_id': str(chunk.document_id),
                    'document_title': getattr(chunk, 'document_title', 'Unknown'),
                    'chunk_index': chunk.chunk_index,
                    'page_number': metadata.get('page_number'),
                    'similarity': 1.0 - distance if isinstance(distance, float) else 0.0,
                    'content_preview': chunk.content[:200] + '...' if len(chunk.content) > 200 else chunk.content
                }
                
                context_parts.append(f"{i}. {source_info}{confidence}\n{chunk.content}\n")
            
            context_parts.append("\n---\n\nIMPORTANT: When citing information from these sources, use the format [N] where N is the source number (1-{len(relevant_chunks)}). Each citation should be placed immediately after the claim it supports.\n\n")
        
        # Build conversation history for OpenAI
        messages = []
        
        # Check if the model supports system messages (o1-mini and o1-preview do not)
        model_name = azure_openai_service.deployment_name.lower()
        supports_system_messages = not any(unsupported in model_name for unsupported in ['o1-mini', 'o1-preview', 'o1'])
        
        # Initialize hidden_context for o1-mini models
        hidden_context = ""
        
        # Base system prompt
        base_prompt = """You are ChatGPT, a large language model based on the GPT-5 model and trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-08-08

Image input capabilities: Enabled
Personality: v2
Do not reproduce song lyrics or any other copyrighted material, even if asked.
You're an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.
Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.
Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.
Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.
Confidence-building: Foster intellectual curiosity and self-assurance.

IMPORTANT: When writing mathematical formulas, equations, or any LaTeX expressions:
- For display equations (centered, on their own line), use: \\begin{equation} ... \\end{equation}
- For inline math (within text), use: \\( ... \\)
- For simple inline math, you can also use: $...$
- Examples:
  * Display: \\begin{equation} E = mc^2 \\end{equation}
  * Inline: The equation \\(E = mc^2\\) shows energy-mass equivalence
  * Simple inline: The famous $E = mc^2$ equation

CITATION FORMATTING - VERY IMPORTANT:
When you reference information from the provided knowledge context, use INLINE citations immediately after the relevant claim:
- Format: "Specific claim or fact[N]" where N is the number of the source
- Place citations directly after the claim they support, not at the end of sentences or paragraphs
- Each citation [N] corresponds to the numbered source in the knowledge context
- Examples:
  * "Einstein developed the theory of relativity[1] which revolutionized physics[1]."
  * "The study found that students performed better[2] when using active learning methods[2]."
  * "Recent research shows[3] that climate change is accelerating[3]."
- Use citations liberally - every factual claim from the knowledge base should be cited
- Multiple facts from the same source should each have their own citation: "Fact A[1] and fact B[1]"

Do not end with opt-in questions or hedging closers. Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I. Ask at most one necessary clarifying question at the start, not the end. If the next step is obvious, do it."""
        
        # Add system message or convert to user message with hidden tags
        if context_parts:
            # Check if we're using fallback results
            if using_fallback:
                context_message = (
                    base_prompt + "\n\n"
                    "You have access to a knowledge base. "
                    "The following context was retrieved but may not be highly relevant to the user's question. "
                    "Use the context cautiously and only if it actually helps answer the question. "
                    "Feel free to indicate when the available information is limited or not directly applicable.\n\n"
                    + "".join(context_parts)
                )
            else:
                context_message = (
                    base_prompt + "\n\n"
                    "You have access to a knowledge base. "
                    "Use the following retrieved context to answer the user's questions. "
                    "If the context is relevant, incorporate it into your response. "
                    "If the context is not relevant, you can ignore it.\n\n"
                    + "".join(context_parts)
                )
        else:
            # No context available, just use base prompt
            context_message = base_prompt
        
        # Add the system message or hidden context
        if supports_system_messages:
            messages.append({"role": "system", "content": context_message})
        else:
            # For o1-mini and similar models, prepare hidden context
            hidden_context = f"<system_context>\n{context_message}</system_context>\n\n"
        
        # Add conversation history (excluding the message we just added)
        for msg in sorted(chat.messages[:-1], key=lambda x: x.created_at):
            messages.append({"role": msg.role, "content": msg.content})
        
        # Add the current user message (with hidden context for o1-mini if needed)
        if hidden_context:
            # Prepend hidden context to user message for o1-mini
            final_user_message = f"{hidden_context}{user_message}"
        else:
            final_user_message = user_message
            
        messages.append({"role": "user", "content": final_user_message})
        
        # Collect document references for the response
        document_references = []
        if relevant_chunks:
            # Group chunks by document and calculate statistics
            doc_stats = {}
            for chunk in relevant_chunks:
                doc_id = str(chunk.document_id) if hasattr(chunk, 'document_id') else None
                if doc_id and doc_id not in doc_stats:
                    doc_stats[doc_id] = {
                        'id': doc_id,
                        'title': getattr(chunk, 'document_title', 'Unknown'),
                        'source_type': getattr(chunk, 'source_type', 'unknown'),
                        'similarities': [],
                        'chunk_count': 0,
                        'chunks_used': []  # NEW: Store individual chunk details
                    }
                
                if doc_id:
                    doc_stats[doc_id]['chunk_count'] += 1
                    distance = getattr(chunk, 'search_distance', 0)
                    similarity = 1.0 - distance if distance is not None else 0.0
                    doc_stats[doc_id]['similarities'].append(similarity)
                    
                    # Parse chunk metadata to get page info
                    chunk_meta = json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}
                    
                    # Add chunk details for highlighting
                    doc_stats[doc_id]['chunks_used'].append({
                        'chunk_id': str(chunk.id),
                        'chunk_index': chunk.chunk_index,
                        'page_number': chunk_meta.get('page_number'),
                        'page_index': chunk_meta.get('page_index'),
                        'text_position': {
                            'start': chunk_meta.get('chunk_start_char'),
                            'end': chunk_meta.get('chunk_end_char')
                        } if chunk_meta.get('chunk_start_char') is not None else None,
                        'similarity': similarity,
                        'content_preview': chunk.content[:200] + '...' if len(chunk.content) > 200 else chunk.content
                    })
            
            # Convert to DocumentReference objects
            for doc_data in doc_stats.values():
                if doc_data['similarities']:
                    document_references.append({
                        'id': doc_data['id'],
                        'title': doc_data['title'],
                        'source_type': doc_data['source_type'],
                        'chunk_count': doc_data['chunk_count'],
                        'max_similarity': max(doc_data['similarities']),
                        'avg_similarity': sum(doc_data['similarities']) / len(doc_data['similarities']),
                        'chunks_used': doc_data['chunks_used'],  # NEW: Include chunk details
                        'tags': []  # TODO: Add tags in future iteration
                    })
        
        # Generate response
        response_content = ""
        
        if use_deep_research:
            # Unified approach: Create assistant message immediately with "running" status
            deep_research_params = {
                "max_concurrent_research_units": max_concurrent_research_units,
                "max_researcher_iterations": max_researcher_iterations,
                "max_react_tool_calls": max_react_tool_calls,
                "max_structured_output_retries": max_structured_output_retries,
                "query": user_message
            }
            
            # Create assistant message using the existing add_message method
            assistant_msg = await ChatService.add_message(
                db,
                chat_id,
                MessageCreate(content="Deep research in progress...", role="assistant")
            )
            
            # Update with deep research specific fields
            result = await db.execute(
                select(MessageModel).where(MessageModel.id == assistant_msg.id)
            )
            db_message = result.scalar_one_or_none()
            if db_message:
                db_message.is_deep_research = True
                db_message.deep_research_status = "running"
                db_message.deep_research_params = json.dumps(deep_research_params)
                await db.commit()
            
            # Start deep research in background (fire and forget)
            asyncio.create_task(ChatService._run_deep_research_background(
                db, assistant_msg.id, user_message, deep_research_params
            ))
            
            # Return immediately - no streaming, no complex generators
            return
        
        # Regular chat flow - stream and save response
        # Use regular Azure OpenAI service for generating the response
        async for chunk in azure_openai_service.generate_chat_completion(messages):
            response_content += chunk
            yield (chunk, None)
        
        # Process citations in response content to create markdown links
        print(f"📖 Processing citations with mapping: {chunk_to_citation_mapping}")
        processed_content = ChatService.process_citations_to_markdown(
            response_content,
            chunk_to_citation_mapping
        )
        print(f"📖 Original content: {response_content[:200]}...")
        print(f"📖 Processed content: {processed_content[:200]}...")
        
        # Save assistant response with processed markdown links
        assistant_msg = await ChatService.add_message(
            db,
            chat_id,
            MessageCreate(content=processed_content, role="assistant")
        )
        
        # Yield final message ID with document references and citation mapping
        yield ("", assistant_msg.id, document_references, chunk_to_citation_mapping)
    
    @staticmethod
    def process_citations_to_markdown(content: str, citation_mapping: dict) -> str:
        """Convert inline citations [1], [2] to markdown links using citation mapping"""
        print(f"📖 process_citations_to_markdown called with content length: {len(content)}")
        print(f"📖 Citation mapping: {citation_mapping}")

        if not citation_mapping:
            print("📖 No citation mapping provided, returning original content")
            return content
        
        import re
        
        def replace_citation(match):
            citation_num = int(match.group(1))
            print(f"📖 Found citation [{citation_num}]")
            citation_data = citation_mapping.get(citation_num)
            print(f"📖 Citation data for [{citation_num}]: {citation_data}")

            if not citation_data:
                # No mapping found, return original citation
                print(f"📖 No mapping found for citation [{citation_num}], keeping original")
                return match.group(0)
            
            # Extract citation data
            document_id = citation_data.get('document_id')
            chunk_id = citation_data.get('chunk_id')
            document_title = citation_data.get('document_title', 'Unknown Document')
            page_number = citation_data.get('page_number')
            
            if not document_id or not chunk_id:
                # Missing required data, return original citation
                return match.group(0)
            
            # Build URL with highlighting parameters
            url = f"/knowledge/{document_id}?chunks={chunk_id}"
            # Don't include pages parameter - let the viewer show all pages and highlight specific chunk

            # Create markdown link: [original_citation](url "title")
            title = f"{document_title}"
            if page_number:
                title += f" (Page {page_number})"

            result = f"[{match.group(0)}]({url} \"{title}\")"
            print(f"📖 Converted [{citation_num}] to markdown: {result}")
            return result
        
        # Pattern to match citations like [1], [2], etc.
        citation_pattern = r'\[(\d+)\]'
        print(f"📖 Looking for citations with pattern: {citation_pattern}")
        processed_content = re.sub(citation_pattern, replace_citation, content)

        print(f"📖 Final processed content: {processed_content[:300]}...")
        return processed_content
    
    @staticmethod
    async def get_or_create_chat(db: AsyncSession, chat_id: Optional[UUID] = None, title: str = "New Chat") -> Chat:
        """Get existing chat or create new one"""
        if chat_id:
            chat = await ChatService.get_chat(db, chat_id)
            if chat:
                return chat

        # Create new chat
        return await ChatService.create_chat(db, ChatCreate(title=title))
    
    @staticmethod
    async def _run_deep_research_background(db: AsyncSession, message_id: UUID, query: str, params: dict):
        """Run deep research in background and update message with results"""
        try:
            # Run the deep research
            research_report = await DeepResearchService.run_deep_research(
                query=query,
                max_concurrent_research_units=params.get("max_concurrent_research_units", 1),
                max_researcher_iterations=params.get("max_researcher_iterations", 1),
                max_react_tool_calls=params.get("max_react_tool_calls", 1),
                max_structured_output_retries=params.get("max_structured_output_retries", 1)
            )
            
            # Update message with results
            result = await db.execute(
                select(MessageModel).where(MessageModel.id == message_id)
            )
            message = result.scalar_one_or_none()
            
            if message:
                message.content = research_report
                message.deep_research_status = "completed"
                message.token_count = azure_openai_service.count_tokens(research_report)
                await db.commit()
                
        except Exception as e:
            # Update message with error
            result = await db.execute(
                select(MessageModel).where(MessageModel.id == message_id)
            )
            message = result.scalar_one_or_none()
            
            if message:
                error_message = f"Deep research failed: {str(e)}"
                message.content = error_message
                message.deep_research_status = "failed"
                message.deep_research_error = str(e)
                await db.commit()
    
    @staticmethod
    async def get_message_status(db: AsyncSession, message_id: UUID) -> Optional[dict]:
        """Get deep research status for a message"""
        # Get the message
        result = await db.execute(
            select(MessageModel)
            .where(MessageModel.id == message_id)
        )
        message = result.scalar_one_or_none()
        
        if not message:
            return None
            
        return {
            "id": str(message.id),
            "content": message.content,
            "is_deep_research": message.is_deep_research,
            "status": message.deep_research_status,
            "error": message.deep_research_error,
            "created_at": message.created_at.isoformat() if message.created_at else None
        }
